{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting house prices is a critical task in the real estate industry, as it directly impacts buyers, sellers, and investors. The goal of this project is to develop a predictive model that accurately estimates the sale price of residential properties in Ames, Iowa. This challenge is particularly significant given the wide range of factors that can influence property prices, from physical characteristics to neighborhood features.\n",
    "\n",
    "The dataset used in this project comes from the Kaggle competition titled “House Prices: Advanced Regression Techniques.” It contains 79 explanatory variables that describe almost every aspect of residential homes in Ames, Iowa. These features include lot size, building dimensions, year built, quality ratings, and more. The competition challenges participants to predict the final sale price of each house based on these variables. The dataset was compiled by Dean De Cock for educational purposes and serves as a modern alternative to the Boston Housing dataset.\n",
    "\n",
    "The project’s objective is to predict the sale price of each house given its features. The predictions are evaluated using Root Mean Squared Error (RMSE) between the logarithm of the predicted and actual sale prices. Using logarithms ensures that errors in predicting both expensive and inexpensive houses are treated equally, preventing bias toward high-priced properties.\n",
    "\n",
    "Regression analysis is at the core of this project, as it is well-suited for predicting continuous numerical values such as house prices. In regression, a mathematical model is constructed to estimate the relationship between one or more independent variables (features) and a dependent variable (sale price). One commonly used regression technique is linear regression, which models the relationship between the dependent and independent variables as a linear combination of the input features. However, more advanced techniques like random forest regression and gradient boosting regression can also be applied to capture complex patterns and interactions within the data. These techniques build multiple decision trees and combine their outputs to achieve better accuracy and robustness.\n",
    "\n",
    "By leveraging feature engineering and advanced regression techniques, this project aims to create a reliable model that accurately predicts house prices, offering valuable insights for real estate professionals and stakeholders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation and analysis\n",
    "import pandas as pd  # Library for data manipulation and analysis\n",
    "import numpy as np  # Library for numerical computations\n",
    "\n",
    "# Import libraries for data visualization\n",
    "import matplotlib.pyplot as plt  # Library for creating static and interactive plots\n",
    "import seaborn as sns  # Library for creating informative and attractive statistical graphics\n",
    "\n",
    "# Enable inline plotting for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Import libraries for machine learning\n",
    "from sklearn.linear_model import LinearRegression  # Library for linear regression models\n",
    "from sklearn.model_selection import train_test_split  # Library for splitting data into training and testing sets\n",
    "from sklearn.metrics import mean_squared_error  # Library for evaluating model performance using mean squared error\n",
    "\n",
    "from sklearn.feature_selection import RFE, SequentialFeatureSelector, VarianceThreshold, SelectKBest, f_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data from CSV files\n",
    "train_df = pd.read_csv('train.csv')  # Load training data into a Pandas DataFrame\n",
    "test_df = pd.read_csv('test.csv')  # Load testing data into a Pandas DataFrame\n",
    "\n",
    "# Display the first few rows of the training data to verify loading and formatting\n",
    "train_df.head()  # Print the first few rows of the training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the training data, including data types and counts of non-null values\n",
    "train_df.info()  # Print a concise summary of the training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the testing data, including data types and counts of non-null values\n",
    "test_df.info()  # Print a concise summary of the testing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for the training data, including mean, std, min, 25%, 50%, 75%, and max values\n",
    "train_df.describe()  # Print summary statistics for the training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for the testing data, including mean, std, min, 25%, 50%, 75%, and max values\n",
    "test_df.describe()  # Print summary statistics for the testing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the count of missing values in each column of the training data\n",
    "print(train_df.isnull().sum())  # Display the total number of null values in each column of the training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the count of missing values in each column of the testing data\n",
    "print(test_df.isnull().sum())  # Display the total number of null values in each column of the testing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns from the training and testing data that have more than 45% missing values\n",
    "train_df = train_df.drop(train_df.columns[(train_df.isnull().sum() / train_df.shape[0] > 0.45)], axis=1)\n",
    "# Drop columns with high missing value rates from the training DataFrame\n",
    "\n",
    "test_df = test_df.drop(test_df.columns[(test_df.isnull().sum() / test_df.shape[0] > 0.45)], axis=1)\n",
    "# Drop columns with high missing value rates from the testing DataFrame\n",
    "\n",
    "# Verify the updated count of missing values in each column of the training and testing data\n",
    "print(train_df.isnull().sum())  # Display the updated total number of null values in each column of the training DataFrame\n",
    "print(test_df.isnull().sum())  # Display the updated total number of null values in each column of the testing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in the 'LotFrontage' column with the median value for both training and testing data\n",
    "train_df['LotFrontage'] = train_df['LotFrontage'].fillna(train_df['LotFrontage'].median())\n",
    "# Fill missing values in the 'LotFrontage' column of the training DataFrame with the median value\n",
    "\n",
    "test_df['LotFrontage'] = test_df['LotFrontage'].fillna(test_df['LotFrontage'].median())\n",
    "# Fill missing values in the 'LotFrontage' column of the testing DataFrame with the median value\n",
    "\n",
    "# Verify the updated count of missing values in each column of the training and testing data\n",
    "print(train_df.isnull().sum())  # Display the updated total number of null values in each column of the training DataFrame\n",
    "print(test_df.isnull().sum())  # Display the updated total number of null values in each column of the testing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in categorical columns with the most frequent value (mode) for both training and testing data\n",
    "train_df = train_df.fillna(train_df.mode().iloc[0])\n",
    "# Fill missing values in the training DataFrame with the most frequent value (mode) for each column\n",
    "\n",
    "test_df = test_df.fillna(test_df.mode().iloc[0])\n",
    "# Fill missing values in the testing DataFrame with the most frequent value (mode) for each column\n",
    "\n",
    "# Verify the updated count of missing values in each column of the training and testing data\n",
    "print(train_df.isnull().sum())  # Display the updated total number of null values in each column of the training DataFrame\n",
    "print(test_df.isnull().sum())  # Display the updated total number of null values in each column of the testing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-informative columns from the training and testing data\n",
    "train_df = train_df.drop(['Id', 'Street', 'LotShape', 'LandContour', 'Utilities', \n",
    "                          'LotConfig', 'LandSlope', 'RoofMatl', 'Heating', 'CentralAir',\n",
    "                          'Electrical', 'Functional', 'GarageQual', 'GarageCond', 'BsmtCond',], axis=1)\n",
    "# Drop columns that do not provide useful information for modeling from the training DataFrame\n",
    "\n",
    "test_df = test_df.drop(['Id', 'Street', 'LotShape', 'LandContour', 'Utilities', \n",
    "                        'LotConfig', 'LandSlope', 'RoofMatl', 'Heating', 'CentralAir',\n",
    "                        'Electrical', 'Functional', 'GarageQual', 'GarageCond', 'BsmtCond',], axis=1)\n",
    "# Drop columns that do not provide useful information for modeling from the testing DataFrame\n",
    "\n",
    "# Display the first few rows of the updated training data to verify changes\n",
    "train_df.head()  # Print the first few rows of the updated training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns 'Condition1' and 'Condition2' from the training and testing data\n",
    "# These columns have dominant values for normal conditions (86% and 99% respectively)\n",
    "# and are suggested to have little to no predictive power\n",
    "train_df = train_df.drop(['Condition1', 'Condition2'], axis=1)\n",
    "# Drop columns 'Condition1' and 'Condition2' from the training DataFrame\n",
    "\n",
    "test_df = test_df.drop(['Condition1', 'Condition2'], axis=1)\n",
    "# Drop columns 'Condition1' and 'Condition2' from the testing DataFrame\n",
    "\n",
    "# Display the first few rows of the updated training data to verify changes\n",
    "train_df.head()  # Print the first few rows of the updated training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns to ensure a 1 or 0 output\n",
    "# This will convert categorical variables into numerical variables that can be used in modeling\n",
    "columns_to_encode = ['BldgType', 'HouseStyle', 'Exterior1st', 'Exterior2nd', 'RoofStyle', 'ExterQual',\n",
    "                     'ExterCond', 'Foundation',  'HeatingQC', 'KitchenQual',\n",
    "                     'GarageType', 'GarageFinish', 'SaleType', 'SaleCondition', 'PavedDrive', 'MSZoning', \n",
    "                     'BsmtQual',  'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',]\n",
    "# Define the list of columns to be one-hot encoded\n",
    "\n",
    "train_df = pd.get_dummies(train_df, columns=columns_to_encode, drop_first=True, dtype=int)\n",
    "# One-hot encode the specified columns in the training DataFrame, dropping the first category to avoid multicollinearity\n",
    "# and ensuring a 1 or 0 output by specifying dtype=int\n",
    "\n",
    "test_df = pd.get_dummies(test_df, columns=columns_to_encode, drop_first=True, dtype=int)\n",
    "# One-hot encode the specified columns in the testing DataFrame, dropping the first category to avoid multicollinearity\n",
    "# and ensuring a 1 or 0 output by specifying dtype=int\n",
    "\n",
    "# Display the first few rows of the updated training data to verify changes\n",
    "train_df.head()  # Print the first few rows of the updated training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the overall mean house price\n",
    "# This will be used as a baseline to compare the mean house prices of different neighborhoods\n",
    "overall_mean = train_df['SalePrice'].mean()\n",
    "# Calculate the mean of the 'SalePrice' column in the training DataFrame\n",
    "\n",
    "# Step 2: Calculate the mean house price for each neighborhood\n",
    "# This will help us understand how the mean house price varies across different neighborhoods\n",
    "neighborhood_means = train_df.groupby('Neighborhood')['SalePrice'].mean()\n",
    "# Group the training DataFrame by the 'Neighborhood' column and calculate the mean of the 'SalePrice' column for each group\n",
    "\n",
    "# Step 3: Map the mean values to the 'Neighborhood' column\n",
    "# This will create a new column 'Neighborhood_Encoded' that contains the mean house price for each neighborhood\n",
    "train_df['Neighborhood_Encoded'] = train_df['Neighborhood'].map(neighborhood_means)\n",
    "# Create a new column 'Neighborhood_Encoded' in the training DataFrame by mapping the 'Neighborhood' column to the mean house prices calculated earlier\n",
    "\n",
    "test_df['Neighborhood_Encoded'] = test_df['Neighborhood'].map(neighborhood_means)\n",
    "# Create a new column 'Neighborhood_Encoded' in the testing DataFrame by mapping the 'Neighborhood' column to the mean house prices calculated earlier\n",
    "\n",
    "# Optional: Track the positive and negative influences\n",
    "# This will help us understand how each neighborhood affects the house price compared to the overall mean\n",
    "neighborhood_influence = neighborhood_means - overall_mean\n",
    "# Calculate the difference between the mean house price of each neighborhood and the overall mean\n",
    "\n",
    "print(\"Neighborhood Influence on Price:\")\n",
    "print(neighborhood_influence.sort_values(ascending=False))\n",
    "# Print the neighborhood influences in descending order (i.e., from most positive to most negative)\n",
    "\n",
    "# Drop the original 'Neighborhood' column after encoding\n",
    "# This is because we no longer need the original 'Neighborhood' column after creating the 'Neighborhood_Encoded' column\n",
    "train_df = train_df.drop(['Neighborhood'], axis=1)\n",
    "# Drop the 'Neighborhood' column from the training DataFrame\n",
    "\n",
    "test_df = test_df.drop(['Neighborhood'], axis=1)\n",
    "# Drop the 'Neighborhood' column from the testing DataFrame\n",
    "\n",
    "# Display the top 5 rows of the updated DataFrame\n",
    "print(train_df[['Neighborhood_Encoded', 'SalePrice']].head())\n",
    "# Print the top 5 rows of the updated training DataFrame, showing the 'Neighborhood_Encoded' and 'SalePrice' columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix of the training DataFrame\n",
    "# This will help us understand the relationships between different columns in the DataFrame\n",
    "train_df.corr()\n",
    "# Calculate the correlation coefficient for each pair of columns in the training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the variance_inflation_factor function from statsmodels\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Select numerical features for VIF calculation\n",
    "# This will exclude the 'SalePrice' column and only consider numerical columns\n",
    "numerical_features = train_df.select_dtypes(include=[np.number]).drop(['SalePrice'], axis=1)\n",
    "# Select numerical columns from the training DataFrame and drop the 'SalePrice' column\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "# This will help us identify features that are highly correlated with each other\n",
    "vif_data = pd.DataFrame()\n",
    "# Create an empty DataFrame to store the VIF results\n",
    "\n",
    "vif_data['Feature'] = numerical_features.columns\n",
    "# Add a column to the DataFrame with the feature names\n",
    "\n",
    "vif_data['VIF'] = [variance_inflation_factor(numerical_features.values, i) for i in range(numerical_features.shape[1])]\n",
    "# Calculate the VIF for each feature and add it to the DataFrame\n",
    "\n",
    "# Print the VIF results in descending order (i.e., from highest to lowest)\n",
    "print(vif_data.sort_values(by='VIF', ascending=False))\n",
    "# Sort the DataFrame by the VIF column in descending order and print the results\n",
    "\n",
    "# Save the VIF results to a CSV file\n",
    "vif_data.to_csv('vif_output.csv', index=False)\n",
    "# Write the DataFrame to a CSV file named 'vif_output.csv' without including the index column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify features with high VIF values (> 25)\n",
    "# This is a common threshold for identifying features with high multicollinearity\n",
    "high_vif = vif_data[vif_data['VIF'] > 25]\n",
    "# Filter the VIF DataFrame to include only rows where the VIF value is greater than 25\n",
    "\n",
    "# Print the features with high VIF values\n",
    "print(high_vif)\n",
    "# Print the filtered DataFrame to display the features with high VIF values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into training and testing sets\n",
    "- also define the feature set (X) and the target variable (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature set (X) and the target variable (y)\n",
    "X = train_df.drop('SalePrice', axis=1)\n",
    "y = train_df['SalePrice']\n",
    "\n",
    "# Split the data into training and testing sets with a 80-20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Variance Threshold (remove features with low variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the selected features from different feature selection methods\n",
    "selected_features = {}\n",
    "\n",
    "# Use VarianceThreshold to select features with a variance threshold of 0.1\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector_variance = VarianceThreshold(threshold=0.1)\n",
    "selector_variance.fit(X_train)\n",
    "\n",
    "# Get the selected features and store them in the dictionary\n",
    "selected_features['Variance Threshold'] = X_train.columns[selector_variance.get_support()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Correlation (check and remove highly correlated features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix of the training data\n",
    "correlation_matrix = X_train.corr().abs()\n",
    "\n",
    "# Get the upper triangle of the correlation matrix\n",
    "# This is done to avoid duplicate comparisons and to only consider the correlations between different features\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Identify the features with high correlation (above 0.9)\n",
    "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "\n",
    "# Select the features that are not highly correlated with each other\n",
    "selected_features['Correlation'] = X_train.columns.difference(high_corr_features).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Statistical Test (SelectKBest with ANOVA F-test)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SelectKBest to select the top 11 features based on the F-regression score\n",
    "# The F-regression score is a statistical test that measures the correlation between each feature and the target variable\n",
    "selector_kbest = SelectKBest(score_func=f_regression, k=8)\n",
    "\n",
    "# Fit the SelectKBest selector to the training data\n",
    "selector_kbest.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features and store them in the dictionary\n",
    "selected_features['Statistical Test (SelectKBest)'] = X_train.columns[selector_kbest.get_support()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Forward Selection with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SequentialFeatureSelector to perform forward selection with linear regression\n",
    "# This selects the top 10 features that have the highest correlation with the target variable\n",
    "lr_model = LinearRegression()  # Create a linear regression model\n",
    "forward_selector = SequentialFeatureSelector(lr_model, n_features_to_select=8, direction='forward', scoring='neg_mean_squared_error', cv=8)\n",
    "\n",
    "# Fit the forward selector to the training data\n",
    "forward_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features and store them in the dictionary\n",
    "selected_features['Forward Selection (Linear Regression)'] = X_train.columns[forward_selector.get_support()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: Recursive Feature Elimination (RFE) with Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Recursive Feature Elimination (RFE) with a decision tree regressor to select the top 11 features\n",
    "dt_model = DecisionTreeRegressor()  # Create a decision tree regressor model\n",
    "rfe_selector = RFE(dt_model, n_features_to_select=8)  # Initialize the RFE selector with the decision tree model and the number of features to select\n",
    "\n",
    "# Fit the RFE selector to the training data\n",
    "rfe_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features and store them in the dictionary\n",
    "selected_features['RFE (Decision Tree Regressor)'] = X_train.columns[rfe_selector.get_support()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Features Selected by Each Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in selected_features.items()]))\n",
    "selected_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "X_selected = ['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'YearRemodAdd', \n",
    "              'BsmtFinSF1', 'BsmtUnfSF', '1stFlrSF', '2ndFlrSF', \n",
    "              'GrLivArea', 'GarageArea', 'MoSold', 'Neighborhood_Encoded']\n",
    "\n",
    "X = train_df[X_selected]  # Extract the actual feature values\n",
    "y = train_df['SalePrice']  # Target variable\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5)  # Corrected to KNeighborsRegressor\n",
    "knn.fit(X_scaled, y)\n",
    "\n",
    "y_pred_train = knn.predict(X_scaled)\n",
    "\n",
    "mae = mean_absolute_error(y, y_pred_train)\n",
    "rmse = mean_squared_error(y, y_pred_train, squared=False)  # RMSE\n",
    "r2 = r2_score(y, y_pred_train)\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Plot\n",
    "At KNN 5, the model is able to capture the underlying patterns in the data with a smaller number of neighbors, resulting in more accurate predictions.\n",
    "As you increase the number of neighbors (K), the model starts to overfit the data, trying to fit the noise rather than the underlying patterns. This results in less accurate predictions.\n",
    "At KNN 15, the model has overfit the data to the point where it's making the least accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of k values to test for the KNeighborsRegressor\n",
    "# These values will be used to determine the number of neighbors to consider when making predictions\n",
    "k_values = [3, 5, 7, 9, 11, 13, 15, 19]\n",
    "\n",
    "# Initialize an empty list to store the mean squared error (MSE) values for each k value\n",
    "mse_k = []\n",
    "\n",
    "# Iterate over each k value in the list\n",
    "for i in k_values:\n",
    "  # Create a new KNeighborsRegressor instance with the current k value\n",
    "  # This will determine the number of neighbors to consider when making predictions\n",
    "  knn = KNeighborsRegressor(n_neighbors=i)\n",
    "  \n",
    "  # Train the KNeighborsRegressor model using the training data\n",
    "  # This will allow the model to learn the relationships between the features and the target variable\n",
    "  knn.fit(X_train, y_train)\n",
    "  \n",
    "  # Use the trained model to make predictions on the test data\n",
    "  # These predictions will be used to calculate the MSE\n",
    "  y_pred = knn.predict(X_test)\n",
    "  \n",
    "  # Calculate the mean squared error (MSE) between the predicted values and the actual values\n",
    "  # This will give us a measure of the model's performance for the current k value\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  \n",
    "  # Append the MSE value to the list of MSE values\n",
    "  # This will allow us to plot the MSE values for each k value later\n",
    "  mse_k.append(mse)\n",
    "\n",
    "# Create a plot of the MSE values against the k values\n",
    "# This will allow us to visualize the relationship between the number of neighbors and the model's performance\n",
    "plt.plot(k_values, mse_k)\n",
    "\n",
    "# Add a label to the x-axis to indicate that it represents the number of neighbors (k)\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "\n",
    "# Add a label to the y-axis to indicate that it represents the mean squared error (MSE)\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "\n",
    "# Add a title to the plot to indicate that it shows the MSE vs k for the KNeighborsRegressor\n",
    "plt.title('KNeighborsRegressor MSE vs k')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X = train_df[X_selected]\n",
    "y = train_df['SalePrice']\n",
    "\n",
    "X = sm.add_constant(X)  # Adds an intercept term to the model\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = ['LotArea', 'OverallQual', 'YearRemodAdd', \n",
    "              'BsmtFinSF1', 'BsmtUnfSF', '1stFlrSF', \n",
    "              'GarageArea', 'Neighborhood_Encoded']\n",
    "# Ensure X_train and X_test only contain the selected features\n",
    "X_train_selected = X_train[X_selected]\n",
    "X_test_selected = X_test[X_selected]\n",
    "\n",
    "# Initialize the Decision Tree Regressor model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model = DecisionTreeRegressor(max_depth=7, random_state=42)\n",
    "\n",
    "# Train the model with the selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Calculate MAE, RMSE, and R² score\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model = DecisionTreeRegressor(max_depth=5, min_samples_leaf=5, min_samples_split=2, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_train_selected, y_train, cv=8, scoring='neg_mean_absolute_error')\n",
    "print(f\"Cross-validated MAE: {-cv_scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [5, 6, 7, 8],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation MAE: {-grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the best parameters\n",
    "best_dt_model = DecisionTreeRegressor(max_depth=8, min_samples_leaf=5, min_samples_split=2)\n",
    "best_dt_model.fit(X_selected, y_train)\n",
    "\n",
    "# Predict on the training and test data\n",
    "y_pred_train = best_dt_model.predict(X_train_selected)\n",
    "y_pred_test = best_dt_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate MAE for both training and test data\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training MAE: {train_mae:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Initialize and fit the regressor\n",
    "dt_reg = DecisionTreeRegressor(max_depth=7, min_samples_leaf=7, min_samples_split=2, random_state=42)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_reg = dt_reg.predict(X_test)\n",
    "\n",
    "# Calculate MAE, RMSE, and R²\n",
    "mae = mean_absolute_error(y_test, y_pred_reg)\n",
    "rmse = mean_squared_error(y_test, y_pred_reg, squared=False)\n",
    "r2 = r2_score(y_test, y_pred_reg)\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Initialize lists to store results\n",
    "mae_results = []\n",
    "rmse_results = []\n",
    "r2_results = []\n",
    "max_depth_range = range(1, 21)  # Adjust the range based on your preference\n",
    "\n",
    "# Loop through different max_depth values\n",
    "for max_depth in max_depth_range:\n",
    "    # Initialize the Decision Tree Regressor with current max_depth\n",
    "    dt_regressor = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    dt_regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = dt_regressor.predict(X_test)\n",
    "\n",
    "    # Calculate MAE, RMSE, and R²\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Append results to lists\n",
    "    mae_results.append(mae)\n",
    "    rmse_results.append(rmse)\n",
    "    r2_results.append(r2)\n",
    "\n",
    "    # Print the results for the current max_depth\n",
    "    print(f\"max_depth: {max_depth}, MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# Find the best max_depth based on lowest MAE\n",
    "best_max_depth = max_depth_range[mae_results.index(min(mae_results))]\n",
    "\n",
    "print(f\"Best max_depth: {best_max_depth}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize the Decision Tree Regressor\n",
    "dt = DecisionTreeRegressor(max_depth=8)\n",
    "\n",
    "# Perform k-fold cross-validation (e.g., 5 folds)\n",
    "cv_scores = cross_val_score(dt, X_scaled, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# The scores will be negative since we're minimizing error, so take the absolute value\n",
    "cv_scores = -cv_scores\n",
    "\n",
    "# Print the mean and standard deviation of the MAE across the folds\n",
    "print(f\"Cross-validated MAE: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
